# -*- coding: utf-8 -*-
"""Phi-3.5 mini with lora working.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XmFFa1s8Q4vEdGUVyLmp57tWk8MeuD_f

Install and Load libraries
"""

#Install the required packages for this project
#!pip install transformers datasets bitsandbytes accelerate peft
#!pip install scikit-learn
#!pip install torch --upgrade
#!pip install evaluate
#!pip install flash-attn
#!pip install wandb
#!pip install logging

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from transformers import EarlyStoppingCallback, TrainerCallback
from peft import get_peft_model, LoraConfig, TaskType
from datasets import Dataset
from sklearn.model_selection import train_test_split
import json
import hashlib
import random
import evaluate
import numpy as np
from huggingface_hub import notebook_login
import time
import math
import warnings
import wandb
import logging
warnings.filterwarnings("ignore", category=FutureWarning, module="torch.utils.checkpoint")
from torch.utils.data import DataLoader


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

torch.backends.cudnn.benchmark = True


# Data loading and preprocessing functions
def load_jsonl(path):
    with open(path, 'r') as file:
        return [json.loads(line) for line in file]

def format_ultrachat_data(data):
    formatted_data = []
    for item in data:
        text = item['text']
        query_start = text.find("### Query:") + len("### Query:")
        response_start = text.find("### Response:") + len("### Response:")
        references_start = text.find("### References:") + len("### References:")

        query = text[query_start:response_start - len("### Response:")].strip()
        response = text[response_start:references_start - len("### References:")].strip()

        prompt_id = hashlib.sha256(query.encode()).hexdigest()

        formatted_item = {
            "prompt": query,
            "prompt_id": prompt_id,
            "messages": [
                {"content": query, "role": "user"},
                {"content": response, "role": "assistant"}
            ]
        }
        formatted_data.append(formatted_item)
    return formatted_data

def collate_and_tokenize(examples, tokenizer, max_length):
    texts = [" ".join([msg['content'] for msg in example['messages']]) for example in examples['data']]

    encoded = tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_tensors="pt"
    )

    encoded['labels'] = encoded['input_ids'].clone()
    return encoded

def prepare_datasets(data_path, tokenizer, max_length=2048):
    try:
        data = load_jsonl(data_path)
    except FileNotFoundError:
        raise FileNotFoundError(f"The file {data_path} was not found. Please check the file path and try again.")

    if not data:
        raise ValueError(f"The file {data_path} is empty or could not be read properly.")

    # Use 70-30 split
    train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)

    train_data_formatted = format_ultrachat_data(train_data)
    test_data_formatted = format_ultrachat_data(test_data)

    train_dataset = Dataset.from_dict({"data": train_data_formatted})
    test_dataset = Dataset.from_dict({"data": test_data_formatted})

    print(f"Dataset size - Train: {len(train_dataset)}, Test: {len(test_dataset)}")

    # Tokenize datasets
    tokenized_train = train_dataset.map(
        lambda examples: collate_and_tokenize(examples, tokenizer, max_length),
        batched=True,
        remove_columns=train_dataset.column_names
    )
    tokenized_test = test_dataset.map(
        lambda examples: collate_and_tokenize(examples, tokenizer, max_length),
        batched=True,
        remove_columns=test_dataset.column_names
    )

    return tokenized_train, tokenized_test

"""Load base model"""

# Set the token as an environment variable
os.environ["HUGGINGFACE_TOKEN"] = "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"

# Login to Hugging Face
#notebook_login()

# Set HF_HOME
os.environ['HF_HOME'] = '/scratch/courses0101/mdhandapani/huggingface_cache'


model_name = "microsoft/Phi-3.5-mini-instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype=torch.float16,
)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name,
                                          add_eos_token=True,
                                          trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.truncation_side = "left"

# Enable gradient checkpointing
model.gradient_checkpointing_enable()

# Define LoRA Config
target_modules = []
for i in range(10):  # Phi-3.5-mini has 10 layers
    target_modules.extend([
        f'model.layers.{i}.self_attn.o_proj',
        f'model.layers.{i}.self_attn.qkv_proj',
        f'model.layers.{i}.mlp.gate_up_proj',
        f'model.layers.{i}.mlp.down_proj',
    ])

config = LoraConfig(
    r=32,
    lora_alpha=32,
    target_modules=target_modules,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Get PEFT model
lora_model = get_peft_model(model, config)

# Print initial trainable parameters
def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}"
    )

print_trainable_parameters(lora_model)

# Prepare datasets
train_dataset, test_dataset = prepare_datasets("/scratch/courses0101/mdhandapani/combined_UnitOps_Training_ZAR.jsonl", tokenizer, max_length=2048)

# Initialize wandb
wandb.init(project="CapstoneProject", entity="shilpasandhya-s229-university-of-western-australia")
logging.basicConfig(level=logging.INFO)

training_args = TrainingArguments(
    output_dir="./phi3_5_mini_instruct_lora_chemical_eng",
    run_name=f"phi3-5-mini-instruct-lora-run-{time.strftime('%Y%m%d-%H%M%S')}",
    num_train_epochs=5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-4,
    weight_decay=0.01,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
    gradient_checkpointing=True,
    optim="adamw_torch",
    logging_dir='./logs',
    logging_strategy="steps",
    logging_steps=1,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=10,
    eval_strategy="steps",
    eval_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,
    fp16=True,
    fp16_full_eval=True,
    max_grad_norm=0.3,
    report_to=["wandb"],
)

early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience=3
)

class DetailedLoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero:
            if 'loss' in logs:
                wandb.log({"train_loss": logs['loss'], "step": state.global_step})
            if 'eval_loss' in logs:
                wandb.log({"eval_loss": logs['eval_loss'], "step": state.global_step})
                perplexity = math.exp(logs['eval_loss'])
                wandb.log({"perplexity": perplexity, "step": state.global_step})

            # Log memory usage
            memory_used = torch.cuda.memory_allocated() / 1e9  # Convert to GB
            wandb.log({"memory_used_gb": memory_used, "step": state.global_step})

def data_collator(examples):
    return tokenizer.pad(examples, padding=True, return_tensors="pt")

train_dataloader = DataLoader(
    train_dataset,
    batch_size=4,
    shuffle=True,
    collate_fn=data_collator
)

eval_dataloader = DataLoader(
    test_dataset,
    batch_size=4,
    collate_fn=data_collator
)

trainer = Trainer(
    model=lora_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
    callbacks=[early_stopping_callback, DetailedLoggingCallback()],
)

# Disable cache to prevent warning, re-enable for inference
model.config.use_cache = False

# Efficiency metrics
start_time = time.time()
start_memory = torch.cuda.memory_allocated()
trainer.train()
end_time = time.time()
end_memory = torch.cuda.memory_allocated()

training_time = end_time - start_time
memory_used = end_memory - start_memory

# Performance evaluation
eval_results = trainer.evaluate()

print(f"Training Time: {training_time:.2f} seconds")
print(f"Memory Used: {memory_used / 1e9:.2f} GB")
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")

# Add this after training
wandb.finish()

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Save the fine-tuned model
lora_model.save_pretrained("./phi3_5_mini_lora_chemical_eng_final")
tokenizer.save_pretrained("./phi3_5_mini_lora_chemical_eng_final")

# Push the model and tokenizer to Hugging Face hub
lora_model.push_to_hub("ShilpaSandhya/phi3_5_mini_lora_chemical_eng")
tokenizer.push_to_hub("ShilpaSandhya/phi3_5_mini_lora_chemical_eng")